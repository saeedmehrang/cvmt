{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38cc88c-6d0d-4933-a983-d06e55a4ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d6393b7-ef7c-4127-934f-68d6111e91d1",
   "metadata": {},
   "source": [
    "# Introduction and Objective\n",
    "## Training with MultiTask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93501c9d-a008-4b2a-a655-5001517d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647f3e61-f764-4f7e-9c07-1ad7f8fd09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ede7ed-71ac-4217-8ef2-d2bd732861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    HDF5MultitaskDataset,\n",
    "    ResizeTransform, \n",
    "    MultitaskCollator,\n",
    "    MultiTaskLandmarkUNetCustom,\n",
    "    nested_dict_to_easydict,\n",
    "    Coord2HeatmapTransform,\n",
    "    CustomToTensor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a96a526-d662-447c-8f1b-49ccfee2d17d",
   "metadata": {},
   "source": [
    "# load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e553e5ae-ffb8-47ca-9445-503de8bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../code_configs/params.yaml\") as f:\n",
    "    PARAMS = yaml.safe_load(f)\n",
    "    PARAMS = nested_dict_to_easydict(PARAMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dbe68e5-8631-4ace-8c28-b3408f52bd06",
   "metadata": {},
   "source": [
    "# Load metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25606a82-607e-49c6-a213-6ead1166f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_hdf(\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, PARAMS.TRAIN.METADATA_TABLE_NAME),\n",
    "    key='df',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578256db-0c4f-49d2-8dd6-5daed2e2a88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_image_filename</th>\n",
       "      <th>harmonized_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dev_set</th>\n",
       "      <th>v_annots_present</th>\n",
       "      <th>f_annots_present</th>\n",
       "      <th>edges_present</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.jpg</td>\n",
       "      <td>041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.jpg</td>\n",
       "      <td>2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.jpg</td>\n",
       "      <td>7201dc2be0b97f59a7901004d6496bbe84c440530776db...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121.jpg</td>\n",
       "      <td>27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_image_filename                                      harmonized_id  \\\n",
       "0                45.jpg  041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...   \n",
       "1                92.jpg  2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...   \n",
       "2                43.jpg  7201dc2be0b97f59a7901004d6496bbe84c440530776db...   \n",
       "3                 7.jpg  2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...   \n",
       "4               121.jpg  27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...   \n",
       "\n",
       "     dataset dev_set  v_annots_present  f_annots_present  edges_present  \\\n",
       "0  dataset_1     NaN              True             False           True   \n",
       "1  dataset_1     NaN              True             False           True   \n",
       "2  dataset_1     NaN              True             False           True   \n",
       "3  dataset_1     NaN              True             False           True   \n",
       "4  dataset_1     NaN              True             False           True   \n",
       "\n",
       "       split  \n",
       "0  undefined  \n",
       "1  undefined  \n",
       "2  undefined  \n",
       "3  undefined  \n",
       "4  undefined  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_table.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afdc1169-690f-41ea-ab70-dadc7d0f5c3b",
   "metadata": {},
   "source": [
    "# DataLoader for task one: Input Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108c65af-0542-4ed6-b700-287411127cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 1\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') , ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_one = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ae8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_one):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "648680a1-aeb7-4aaf-9ffa-8ef68cfc6b42",
   "metadata": {},
   "source": [
    "# DataLoader for task two: Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9325d7a-c53a-4ea0-b28d-f4ff765a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 2\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['edges_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_two = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4223f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'edges'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_two):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62929050-b1f4-4fd0-937d-a9ce3bdee51b",
   "metadata": {},
   "source": [
    "# DataLoader for task three: Vertebral Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728a4a20-a67a-414c-ba82-ebc9438deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 3\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['v_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_three = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0858658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'v_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([13, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([4, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_three):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c1a823-e32e-47da-b07c-a8e31cd41dc9",
   "metadata": {},
   "source": [
    "# DataLoader for task four: Facial Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21627264-7ec1-4978-86cf-e3a7ddb9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 4\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['f_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_four = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22aba645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'f_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([19, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([4, 19, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_four):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5052f338-0077-45a7-8ace-c6302a18b48f",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f14e73-1816-4ee4-8a56-81fab146179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /home/samehr/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n",
      "100%|██████████| 74.4M/74.4M [00:09<00:00, 8.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskLandmarkUNetCustom(\n",
    "    in_channels=1,\n",
    "    out_channels1=1,\n",
    "    out_channels2=1,\n",
    "    out_channels3=13,\n",
    "    out_channels4=19,\n",
    "    enc_chan_multiplier=1,\n",
    "    dec_chan_multiplier=1,\n",
    "    backbone_encoder=\"efficientnet-b4\",\n",
    "    backbone_weights=\"imagenet\",\n",
    "    freeze_backbone=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b3b7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = PARAMS.MODEL.PARAMS\n",
    "model = MultiTaskLandmarkUNetCustom(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805bb3d8-8c07-4d6f-97f0-b484508b28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters:  450266\n"
     ]
    }
   ],
   "source": [
    "# count the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75fb31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1, 1, 256, 256)\n",
    "image /= image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a786b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "out = model(image, task_id=3)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1d1ba8-7d69-43a5-9ebd-952131706010",
   "metadata": {},
   "source": [
    "# Test Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9565c527-b628-4f18-b3b1-262020761216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    trainer_v_landmarks_single_task,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26fe04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                        | Params\n",
      "----------------------------------------------------------\n",
      "0 | model     | MultiTaskLandmarkUNetCustom | 18.0 M\n",
      "1 | train_mse | MeanSquaredError            | 0     \n",
      "2 | val_mse   | MeanSquaredError            | 0     \n",
      "----------------------------------------------------------\n",
      "450 K     Trainable params\n",
      "17.5 M    Non-trainable params\n",
      "18.0 M    Total params\n",
      "71.992    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:10<00:10, 10.43s/it]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 245, in __call__\n    out = self.__supervised_training_collation(batch)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in __supervised_training_collation\n    batch = [sample for sample in batch if sample[self.label_key] != None]\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in <listcomp>\n    batch = [sample for sample in batch if sample[self.label_key] != None]\nKeyError: 'v_landmarks'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_v_landmarks_single_task()\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/notebooks/ml/utils.py:1008\u001b[0m, in \u001b[0;36mtrainer_v_landmarks_single_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1003\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(        \n\u001b[1;32m   1004\u001b[0m     max_epochs\u001b[39m=\u001b[39mPARAMS\u001b[39m.\u001b[39mTRAIN\u001b[39m.\u001b[39mMAX_EPOCHS,\n\u001b[1;32m   1005\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1006\u001b[0m )\n\u001b[1;32m   1007\u001b[0m \u001b[39m# run the training\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1009\u001b[0m     pl_model,\n\u001b[1;32m   1010\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m   1011\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[1;32m   1013\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:976\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m--> 976\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m    978\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1005\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1002\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1004\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1007\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1009\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:108\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m         batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    109\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    110\u001b[0m         \u001b[39mif\u001b[39;00m previous_dataloader_idx \u001b[39m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    111\u001b[0m             \u001b[39m# the dataloader has changed, notify the logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:136\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:150\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_profiler()\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    151\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:276\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    275\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    278\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:122\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterators[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_idx])\n\u001b[1;32m    123\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 245, in __call__\n    out = self.__supervised_training_collation(batch)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in __supervised_training_collation\n    batch = [sample for sample in batch if sample[self.label_key] != None]\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in <listcomp>\n    batch = [sample for sample in batch if sample[self.label_key] != None]\nKeyError: 'v_landmarks'\n"
     ]
    }
   ],
   "source": [
    "trainer_v_landmarks_single_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b032c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader\n",
    "\n",
    "task_config = PARAMS.TRAIN.SINGLE_TASK\n",
    "task_id = task_config.TASK_ID\n",
    "batch_size = task_config.BATCH_SIZE\n",
    "\n",
    "train_dataloader = create_dataloader(\n",
    "    task_id=task_id,\n",
    "    batch_size=batch_size,\n",
    "    split='train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf14e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "1 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "2 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "3 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 245, in __call__\n    out = self.__supervised_training_collation(batch)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in __supervised_training_collation\n    batch = [sample for sample in batch if sample[self.label_key] != None]\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in <listcomp>\n    batch = [sample for sample in batch if sample[self.label_key] != None]\nKeyError: 'v_landmarks'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, sample_batched \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i_batch, sample_batched[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msize(),\n\u001b[1;32m      3\u001b[0m           sample_batched[\u001b[39m'\u001b[39m\u001b[39mv_landmarks\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 245, in __call__\n    out = self.__supervised_training_collation(batch)\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in __supervised_training_collation\n    batch = [sample for sample in batch if sample[self.label_key] != None]\n  File \"/home/samehr/Desktop/cephal/cvmt/notebooks/ml/utils.py\", line 200, in <listcomp>\n    batch = [sample for sample in batch if sample[self.label_key] != None]\nKeyError: 'v_landmarks'\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['v_landmarks'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169e927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
