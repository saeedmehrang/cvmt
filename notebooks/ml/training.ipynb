{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38cc88c-6d0d-4933-a983-d06e55a4ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6393b7-ef7c-4127-934f-68d6111e91d1",
   "metadata": {},
   "source": [
    "# Introduction and Objective\n",
    "## Training with MultiTask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93501c9d-a008-4b2a-a655-5001517d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647f3e61-f764-4f7e-9c07-1ad7f8fd09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ede7ed-71ac-4217-8ef2-d2bd732861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    HDF5MultitaskDataset,\n",
    "    ResizeTransform, \n",
    "    MultitaskCollator,\n",
    "    MultiTaskLandmarkUNet1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96a526-d662-447c-8f1b-49ccfee2d17d",
   "metadata": {},
   "source": [
    "# load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e553e5ae-ffb8-47ca-9445-503de8bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../code_configs/params.yaml\") as f:\n",
    "    params = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe68e5-8631-4ace-8c28-b3408f52bd06",
   "metadata": {},
   "source": [
    "# Load metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25606a82-607e-49c6-a213-6ead1166f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_hdf(\n",
    "    os.path.join(params['PRIMARY_DATA_DIRECTORY'], params['METADATA_TABLE_NAME']),\n",
    "    key='df',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "578256db-0c4f-49d2-8dd6-5daed2e2a88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_image_filename</th>\n",
       "      <th>harmonized_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dev_set</th>\n",
       "      <th>v_annots_present</th>\n",
       "      <th>f_annots_present</th>\n",
       "      <th>edges_present</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.jpg</td>\n",
       "      <td>041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.jpg</td>\n",
       "      <td>2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.jpg</td>\n",
       "      <td>7201dc2be0b97f59a7901004d6496bbe84c440530776db...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121.jpg</td>\n",
       "      <td>27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_image_filename                                      harmonized_id  \\\n",
       "0                45.jpg  041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...   \n",
       "1                92.jpg  2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...   \n",
       "2                43.jpg  7201dc2be0b97f59a7901004d6496bbe84c440530776db...   \n",
       "3                 7.jpg  2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...   \n",
       "4               121.jpg  27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...   \n",
       "\n",
       "     dataset dev_set  v_annots_present  f_annots_present  edges_present  \\\n",
       "0  dataset_1     NaN              True             False           True   \n",
       "1  dataset_1     NaN              True             False           True   \n",
       "2  dataset_1     NaN              True             False           True   \n",
       "3  dataset_1     NaN              True             False           True   \n",
       "4  dataset_1     NaN              True             False           True   \n",
       "\n",
       "       split  \n",
       "0  undefined  \n",
       "1  undefined  \n",
       "2  undefined  \n",
       "3  undefined  \n",
       "4  undefined  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc1169-690f-41ea-ab70-dadc7d0f5c3b",
   "metadata": {},
   "source": [
    "# DataLoader for task one: Input Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "108c65af-0542-4ed6-b700-287411127cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 1\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') , ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(params['PRIMARY_DATA_DIRECTORY'], file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(params['TARGET_IMAGE_SIZE'])),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_one = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3ae8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_one):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648680a1-aeb7-4aaf-9ffa-8ef68cfc6b42",
   "metadata": {},
   "source": [
    "# DataLoader for task two: Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9325d7a-c53a-4ea0-b28d-f4ff765a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 2\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['edges_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(params['PRIMARY_DATA_DIRECTORY'], file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(params['TARGET_IMAGE_SIZE'])),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_two = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4223f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'edges'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_two):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62929050-b1f4-4fd0-937d-a9ce3bdee51b",
   "metadata": {},
   "source": [
    "# DataLoader for task three: Vertebral Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "728a4a20-a67a-414c-ba82-ebc9438deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 3\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['v_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(params['PRIMARY_DATA_DIRECTORY'], file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(params['TARGET_IMAGE_SIZE'])),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_three = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0858658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'v_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([13, 2])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([4, 13, 2])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_three):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1a823-e32e-47da-b07c-a8e31cd41dc9",
   "metadata": {},
   "source": [
    "# DataLoader for task four: Facial Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21627264-7ec1-4978-86cf-e3a7ddb9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 4\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['f_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(params['PRIMARY_DATA_DIRECTORY'], file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(params['TARGET_IMAGE_SIZE'])),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_four = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22aba645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'f_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([19, 2])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([4, 19, 2])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_four):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052f338-0077-45a7-8ace-c6302a18b48f",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21f14e73-1816-4ee4-8a56-81fab146179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskLandmarkUNet1(\n",
    "    in_channels=3,\n",
    "    out_channels1=1,\n",
    "    out_channels2=1,\n",
    "    out_channels3=13,\n",
    "    out_channels4=19,\n",
    "    enc_chan_multiplier=1,\n",
    "    dec_chan_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "805bb3d8-8c07-4d6f-97f0-b484508b28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters:  407442\n"
     ]
    }
   ],
   "source": [
    "# count the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d1ba8-7d69-43a5-9ebd-952131706010",
   "metadata": {},
   "source": [
    "# Test Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26258b3f-48ca-44ea-9b9b-10f74676338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_fc = nn.Linear(10, 20)\n",
    "        self.output_layer1 = nn.Linear(20, 2)\n",
    "        self.output_layer2 = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        x = torch.relu(self.shared_fc(x))\n",
    "        if task_id == 1:\n",
    "            print(\"task_id \", task_id)\n",
    "            output1 = self.output_layer1(x)\n",
    "            return output1\n",
    "        elif task_id == 2:\n",
    "            print(\"task_id \", task_id)\n",
    "            output2 = self.output_layer2(x)\n",
    "            return output2\n",
    "\n",
    "\n",
    "class TwoLayerNetworkTask1(pl.LightningModule):\n",
    "    def __init__(self, my_model, task_id):\n",
    "        super().__init__()\n",
    "        self.my_model = my_model\n",
    "        self.task_id = task_id\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.to(torch.float32)\n",
    "        output = self.my_model(x, task_id=self.task_id)\n",
    "        loss = nn.MSELoss()(output, y)\n",
    "        self.log(f'train_loss_{self.task_id}', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "class TwoLayerNetworkTask2(pl.LightningModule):\n",
    "    def __init__(self, my_model, task_id):\n",
    "        super().__init__()\n",
    "        self.my_model = my_model\n",
    "        self.task_id = task_id\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.to(torch.float32)\n",
    "        output = self.my_model(x, task_id=self.task_id)\n",
    "        loss = nn.MSELoss()(output, y)\n",
    "        self.log(f'train_loss_{self.task_id}', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9565c527-b628-4f18-b3b1-262020761216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples, input_size, task_id):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_size = input_size\n",
    "        self.task_id = task_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.randn(self.input_size)\n",
    "        if self.task_id == 1:\n",
    "            y1 = torch.randint(0, 2, (2,))\n",
    "            return x, y1\n",
    "        elif self.task_id == 2:\n",
    "            y2 = torch.randint(0, 3, (3,))\n",
    "            return x, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cc9583-5c1d-42c2-87e4-26d302489303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dummy dataset\n",
    "dataset1 = DummyDataset(num_samples=100, input_size=10, task_id=1)\n",
    "dataset2 = DummyDataset(num_samples=100, input_size=10, task_id=2)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size1 = 8\n",
    "batch_size2 = 3\n",
    "dataloader1 = DataLoader(dataset1, batch_size=batch_size1, shuffle=True)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=batch_size2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd8468e-8e5c-48ba-9f42-2dcb24e89210",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/samehr/Desktop/cephal/cvmt/notebooks/ml/lightning_logs\n",
      "\n",
      "  | Name     | Type    | Params\n",
      "-------------------------------------\n",
      "0 | my_model | MyModel | 325   \n",
      "-------------------------------------\n",
      "325       Trainable params\n",
      "0         Non-trainable params\n",
      "325       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                     | 0/2 [00:00<?, ?it/s]task_id  1\n",
      "Epoch 0:  50%|██████████████████████████████████████████████████                                                  | 1/2 [00:00<00:00, 60.06it/s, v_num=0]task_id  1\n",
      "Epoch 1:   0%|                                                                                                            | 0/2 [00:00<?, ?it/s, v_num=0]task_id  1\n",
      "Epoch 1:  50%|██████████████████████████████████████████████████                                                  | 1/2 [00:00<00:00, 62.85it/s, v_num=0]task_id  1\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 71.08it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 55.36it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type    | Params\n",
      "-------------------------------------\n",
      "0 | my_model | MyModel | 325   \n",
      "-------------------------------------\n",
      "325       Trainable params\n",
      "0         Non-trainable params\n",
      "325       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   0%|                                                                                                                     | 0/5 [00:00<?, ?it/s]task_id  2\n",
      "Epoch 0:  20%|████████████████████                                                                                | 1/5 [00:00<00:00, 85.28it/s, v_num=1]task_id  2\n",
      "Epoch 0:  40%|████████████████████████████████████████                                                            | 2/5 [00:00<00:00, 68.01it/s, v_num=1]task_id  2\n",
      "Epoch 0:  60%|████████████████████████████████████████████████████████████                                        | 3/5 [00:00<00:00, 78.92it/s, v_num=1]task_id  2\n",
      "Epoch 0:  80%|████████████████████████████████████████████████████████████████████████████████                    | 4/5 [00:00<00:00, 74.09it/s, v_num=1]task_id  2\n",
      "Epoch 1:   0%|                                                                                                            | 0/5 [00:00<?, ?it/s, v_num=1]task_id  2\n",
      "Epoch 1:  20%|████████████████████                                                                                | 1/5 [00:00<00:00, 78.80it/s, v_num=1]task_id  2\n",
      "Epoch 1:  40%|████████████████████████████████████████                                                            | 2/5 [00:00<00:00, 79.53it/s, v_num=1]task_id  2\n",
      "Epoch 1:  60%|████████████████████████████████████████████████████████████                                        | 3/5 [00:00<00:00, 72.39it/s, v_num=1]task_id  2\n",
      "Epoch 1:  80%|████████████████████████████████████████████████████████████████████████████████                    | 4/5 [00:00<00:00, 79.22it/s, v_num=1]task_id  2\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 73.83it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 62.19it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "# Create instances of the PyTorch Lightning modules with the same model\n",
    "module1 = TwoLayerNetworkTask1(model, task_id=1)\n",
    "module2 = TwoLayerNetworkTask2(model, task_id=2)\n",
    "\n",
    "# Create the Trainer objects and train the modules\n",
    "trainer1 = pl.Trainer(limit_train_batches=2, max_epochs=2, log_every_n_steps=1)\n",
    "trainer1.fit(module1, dataloader1)\n",
    "\n",
    "trainer2 = pl.Trainer(limit_train_batches=5, max_epochs=2, log_every_n_steps=1)\n",
    "trainer2.fit(module2, dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9810d66-1c3e-4f1d-8c2f-5629c41bd80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefdcf4-623c-46dd-a46b-ea88b5c2c282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
