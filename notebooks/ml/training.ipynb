{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38cc88c-6d0d-4933-a983-d06e55a4ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6393b7-ef7c-4127-934f-68d6111e91d1",
   "metadata": {},
   "source": [
    "# Introduction and Objective\n",
    "## Training with MultiTask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93501c9d-a008-4b2a-a655-5001517d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647f3e61-f764-4f7e-9c07-1ad7f8fd09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ede7ed-71ac-4217-8ef2-d2bd732861a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import (\n",
    "    HDF5MultitaskDataset,\n",
    "    ResizeTransform, \n",
    "    MultitaskCollator,\n",
    "    MultiTaskLandmarkUNetCustom,\n",
    "    nested_dict_to_easydict,\n",
    "    Coord2HeatmapTransform,\n",
    "    CustomToTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96a526-d662-447c-8f1b-49ccfee2d17d",
   "metadata": {},
   "source": [
    "# load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e553e5ae-ffb8-47ca-9445-503de8bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../code_configs/params.yaml\") as f:\n",
    "    PARAMS = yaml.safe_load(f)\n",
    "    PARAMS = nested_dict_to_easydict(PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe68e5-8631-4ace-8c28-b3408f52bd06",
   "metadata": {},
   "source": [
    "# Load metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25606a82-607e-49c6-a213-6ead1166f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_hdf(\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, PARAMS.TRAIN.METADATA_TABLE_NAME),\n",
    "    key='df',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578256db-0c4f-49d2-8dd6-5daed2e2a88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_annots_present</th>\n",
       "      <th>f_annots_present</th>\n",
       "      <th>edges_present</th>\n",
       "      <th>f_annots_rows</th>\n",
       "      <th>f_annots_cols</th>\n",
       "      <th>harmonized_id</th>\n",
       "      <th>v_annots_2_rows</th>\n",
       "      <th>v_annots_2_cols</th>\n",
       "      <th>v_annots_3_rows</th>\n",
       "      <th>v_annots_3_cols</th>\n",
       "      <th>v_annots_4_rows</th>\n",
       "      <th>v_annots_4_cols</th>\n",
       "      <th>source_image_filename</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dev_set</th>\n",
       "      <th>valid</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.jpg</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.jpg</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7201dc2be0b97f59a7901004d6496bbe84c440530776db...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.jpg</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.jpg</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.jpg</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v_annots_present  f_annots_present  edges_present f_annots_rows  \\\n",
       "0              True             False           True           NaN   \n",
       "1              True             False           True           NaN   \n",
       "2              True             False           True           NaN   \n",
       "3              True             False           True           NaN   \n",
       "4              True             False           True           NaN   \n",
       "\n",
       "  f_annots_cols                                      harmonized_id  \\\n",
       "0           NaN  041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...   \n",
       "1           NaN  2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...   \n",
       "2           NaN  7201dc2be0b97f59a7901004d6496bbe84c440530776db...   \n",
       "3           NaN  2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...   \n",
       "4           NaN  27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...   \n",
       "\n",
       "   v_annots_2_rows  v_annots_2_cols  v_annots_3_rows  v_annots_3_cols  \\\n",
       "0              3.0              2.0              5.0              2.0   \n",
       "1              3.0              2.0              5.0              2.0   \n",
       "2              3.0              2.0              5.0              2.0   \n",
       "3              3.0              2.0              5.0              2.0   \n",
       "4              3.0              2.0              5.0              2.0   \n",
       "\n",
       "   v_annots_4_rows  v_annots_4_cols source_image_filename    dataset dev_set  \\\n",
       "0              5.0              2.0                45.jpg  dataset_1     NaN   \n",
       "1              5.0              2.0                92.jpg  dataset_1     NaN   \n",
       "2              5.0              2.0                43.jpg  dataset_1     NaN   \n",
       "3              5.0              2.0                 7.jpg  dataset_1     NaN   \n",
       "4              5.0              2.0               121.jpg  dataset_1     NaN   \n",
       "\n",
       "   valid      split  \n",
       "0   True  undefined  \n",
       "1   True  undefined  \n",
       "2   True  undefined  \n",
       "3   True  undefined  \n",
       "4   True  undefined  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc1169-690f-41ea-ab70-dadc7d0f5c3b",
   "metadata": {},
   "source": [
    "# DataLoader for task one: Input Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108c65af-0542-4ed6-b700-287411127cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 1\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') , ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_one = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ae8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_one):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648680a1-aeb7-4aaf-9ffa-8ef68cfc6b42",
   "metadata": {},
   "source": [
    "# DataLoader for task two: Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9325d7a-c53a-4ea0-b28d-f4ff765a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 2\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['edges_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_two = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4223f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'edges'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_two):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62929050-b1f4-4fd0-937d-a9ce3bdee51b",
   "metadata": {},
   "source": [
    "# DataLoader for task three: Vertebral Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728a4a20-a67a-414c-ba82-ebc9438deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 3\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['v_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_three = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0858658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'v_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([13, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([4, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_three):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1a823-e32e-47da-b07c-a8e31cd41dc9",
   "metadata": {},
   "source": [
    "# DataLoader for task four: Facial Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21627264-7ec1-4978-86cf-e3a7ddb9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 4\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['f_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_four = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22aba645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'f_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([19, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([4, 19, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_four):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052f338-0077-45a7-8ace-c6302a18b48f",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f14e73-1816-4ee4-8a56-81fab146179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskLandmarkUNetCustom(\n",
    "    in_channels=1,\n",
    "    out_channels1=1,\n",
    "    out_channels2=1,\n",
    "    out_channels3=13,\n",
    "    out_channels4=19,\n",
    "    enc_chan_multiplier=1,\n",
    "    dec_chan_multiplier=1,\n",
    "    backbone_encoder=\"efficientnet-b4\",\n",
    "    backbone_weights=\"imagenet\",\n",
    "    freeze_backbone=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3b7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = PARAMS.MODEL.PARAMS\n",
    "model = MultiTaskLandmarkUNetCustom(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805bb3d8-8c07-4d6f-97f0-b484508b28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters:  450266\n"
     ]
    }
   ],
   "source": [
    "# count the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75fb31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1, 1, 256, 256)\n",
    "image /= image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a786b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "out = model(image, task_id=3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d1ba8-7d69-43a5-9ebd-952131706010",
   "metadata": {},
   "source": [
    "# Test Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9565c527-b628-4f18-b3b1-262020761216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    trainer_v_landmarks_single_task,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26fe04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                        | Params\n",
      "----------------------------------------------------------\n",
      "0 | model     | MultiTaskLandmarkUNetCustom | 18.0 M\n",
      "1 | train_mse | MeanSquaredError            | 0     \n",
      "2 | val_mse   | MeanSquaredError            | 0     \n",
      "----------------------------------------------------------\n",
      "450 K     Trainable params\n",
      "17.5 M    Non-trainable params\n",
      "18.0 M    Total params\n",
      "71.992    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 17/17 [01:44<00:00,  6.12s/it, v_num=11]                                 \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                   | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                      | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██████████▎                                                   | 1/6 [00:05<00:27,  5.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|████████████████████▋                                         | 2/6 [00:10<00:21,  5.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|███████████████████████████████                               | 3/6 [00:16<00:16,  5.61s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████████████████████████████████▎                    | 4/6 [00:21<00:10,  5.35s/it]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████████████████████████████████████████▋          | 5/6 [00:25<00:05,  5.12s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 17/17 [02:13<00:00,  7.83s/it, v_num=11]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████████████████▏       | 15/17 [01:38<00:13,  6.56s/it, v_num=11]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer_v_landmarks_single_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b032c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader\n",
    "\n",
    "task_config = PARAMS.TRAIN.SINGLE_TASK\n",
    "task_id = task_config.TASK_ID\n",
    "batch_size = task_config.BATCH_SIZE\n",
    "\n",
    "train_dataloader = create_dataloader(\n",
    "    task_id=task_id,\n",
    "    batch_size=batch_size,\n",
    "    split='train',\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf14e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "1 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "2 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "3 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "4 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "5 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "6 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "7 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "8 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "9 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "10 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "11 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "12 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "13 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "14 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "15 torch.Size([16, 1, 256, 256]) torch.Size([16, 13, 256, 256])\n",
      "16 torch.Size([6, 1, 256, 256]) torch.Size([6, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['v_landmarks'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169e927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
