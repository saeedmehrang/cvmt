{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38cc88c-6d0d-4933-a983-d06e55a4ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6393b7-ef7c-4127-934f-68d6111e91d1",
   "metadata": {},
   "source": [
    "# Introduction and Objective\n",
    "## Training with MultiTask Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93501c9d-a008-4b2a-a655-5001517d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647f3e61-f764-4f7e-9c07-1ad7f8fd09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ede7ed-71ac-4217-8ef2-d2bd732861a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import (\n",
    "    HDF5MultitaskDataset,\n",
    "    ResizeTransform, \n",
    "    MultitaskCollator,\n",
    "    MultiTaskLandmarkUNetCustom,\n",
    "    nested_dict_to_easydict,\n",
    "    Coord2HeatmapTransform,\n",
    "    CustomToTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96a526-d662-447c-8f1b-49ccfee2d17d",
   "metadata": {},
   "source": [
    "# load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e553e5ae-ffb8-47ca-9445-503de8bb4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../code_configs/params.yaml\") as f:\n",
    "    PARAMS = yaml.safe_load(f)\n",
    "    PARAMS = nested_dict_to_easydict(PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe68e5-8631-4ace-8c28-b3408f52bd06",
   "metadata": {},
   "source": [
    "# Load metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25606a82-607e-49c6-a213-6ead1166f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_hdf(\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, PARAMS.TRAIN.METADATA_TABLE_NAME),\n",
    "    key='df',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578256db-0c4f-49d2-8dd6-5daed2e2a88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_image_filename</th>\n",
       "      <th>harmonized_id</th>\n",
       "      <th>dataset</th>\n",
       "      <th>dev_set</th>\n",
       "      <th>v_annots_present</th>\n",
       "      <th>f_annots_present</th>\n",
       "      <th>edges_present</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.jpg</td>\n",
       "      <td>041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.jpg</td>\n",
       "      <td>2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.jpg</td>\n",
       "      <td>7201dc2be0b97f59a7901004d6496bbe84c440530776db...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121.jpg</td>\n",
       "      <td>27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...</td>\n",
       "      <td>dataset_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_image_filename                                      harmonized_id  \\\n",
       "0                45.jpg  041281ee7fb89f6835a71c309b3b503e3d5a68fc46a608...   \n",
       "1                92.jpg  2cfa37a69916c8a45a51bb8beeb04425e07d2a22f694e0...   \n",
       "2                43.jpg  7201dc2be0b97f59a7901004d6496bbe84c440530776db...   \n",
       "3                 7.jpg  2cd4487c03c72d1016ea0a72d1b21eb987878c90ae9eff...   \n",
       "4               121.jpg  27624a6eb37bbc8aafabe2075f423d573b189eae6f23fb...   \n",
       "\n",
       "     dataset dev_set  v_annots_present  f_annots_present  edges_present  \\\n",
       "0  dataset_1     NaN              True             False           True   \n",
       "1  dataset_1     NaN              True             False           True   \n",
       "2  dataset_1     NaN              True             False           True   \n",
       "3  dataset_1     NaN              True             False           True   \n",
       "4  dataset_1     NaN              True             False           True   \n",
       "\n",
       "       split  \n",
       "0  undefined  \n",
       "1  undefined  \n",
       "2  undefined  \n",
       "3  undefined  \n",
       "4  undefined  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc1169-690f-41ea-ab70-dadc7d0f5c3b",
   "metadata": {},
   "source": [
    "# DataLoader for task one: Input Image Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108c65af-0542-4ed6-b700-287411127cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 1\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') , ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_one = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ae8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_one):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648680a1-aeb7-4aaf-9ffa-8ef68cfc6b42",
   "metadata": {},
   "source": [
    "# DataLoader for task two: Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9325d7a-c53a-4ea0-b28d-f4ff765a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 2\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['edges_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_two = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4223f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'edges'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "edges\n",
      "torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_two):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62929050-b1f4-4fd0-937d-a9ce3bdee51b",
   "metadata": {},
   "source": [
    "# DataLoader for task three: Vertebral Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728a4a20-a67a-414c-ba82-ebc9438deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 3\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['v_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_three = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0858658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'v_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([13, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "v_landmarks\n",
      "torch.Size([4, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_three):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1a823-e32e-47da-b07c-a8e31cd41dc9",
   "metadata": {},
   "source": [
    "# DataLoader for task four: Facial Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21627264-7ec1-4978-86cf-e3a7ddb9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the task id\n",
    "task_id = 4\n",
    "\n",
    "# create the right list of paths\n",
    "train_file_list = metadata_table.loc[\n",
    "    (metadata_table['split']=='train') & (metadata_table['f_annots_present']==True), ['harmonized_id']\n",
    "].to_numpy().ravel().tolist()\n",
    "\n",
    "train_file_list = [\n",
    "    os.path.join(PARAMS.PRIMARY_DATA_DIRECTORY, file_path+'.hdf5') for file_path in train_file_list\n",
    "]\n",
    "\n",
    "# instantiate the transforms\n",
    "my_transforms = transforms.Compose([\n",
    "    ResizeTransform(tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE)),\n",
    "    Coord2HeatmapTransform(\n",
    "        tuple(PARAMS.TRAIN.TARGET_IMAGE_SIZE),\n",
    "        PARAMS.TRAIN.GAUSSIAN_COORD2HEATMAP_STD\n",
    "    ),\n",
    "    CustomToTensor(),\n",
    "])\n",
    "\n",
    "# instantiate the dataset and dataloader objects\n",
    "train_dataset = HDF5MultitaskDataset(\n",
    "    file_paths=train_file_list,\n",
    "    task_id=task_id,\n",
    "    transforms=my_transforms,\n",
    ")\n",
    "collator_task = MultitaskCollator(\n",
    "    task_id=task_id,\n",
    ")\n",
    "dataloader_four = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator_task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22aba645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Sanity check dataset object!\n",
      "dict_keys(['image', 'f_landmarks'])\n",
      "\n",
      "image\n",
      "torch.Size([1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([19, 256, 256])\n",
      "\n",
      "-- Sanity check dataloader object!\n",
      "batch_ndx  0\n",
      "\n",
      "image\n",
      "torch.Size([4, 1, 256, 256])\n",
      "\n",
      "f_landmarks\n",
      "torch.Size([4, 19, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset and dataloader\n",
    "\n",
    "# dataset\n",
    "print(\"-- Sanity check dataset object!\")\n",
    "dataset_iter = iter(train_dataset)\n",
    "for batch in dataset_iter:\n",
    "    print(batch.keys())\n",
    "    for k, v in batch.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "# data loader\n",
    "print(\"-- Sanity check dataloader object!\")\n",
    "for batch_ndx, sample in enumerate(dataloader_four):\n",
    "    print(\"batch_ndx \", batch_ndx)\n",
    "    for k, v in sample.items():\n",
    "        print()\n",
    "        print(k,)\n",
    "        print(v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052f338-0077-45a7-8ace-c6302a18b48f",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f14e73-1816-4ee4-8a56-81fab146179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskLandmarkUNetCustom(\n",
    "    in_channels=1,\n",
    "    out_channels1=1,\n",
    "    out_channels2=1,\n",
    "    out_channels3=13,\n",
    "    out_channels4=19,\n",
    "    enc_chan_multiplier=1,\n",
    "    dec_chan_multiplier=1,\n",
    "    backbone_encoder=\"efficientnet-b4\",\n",
    "    backbone_weights=\"imagenet\",\n",
    "    freeze_backbone=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3b7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = PARAMS.MODEL.PARAMS\n",
    "model = MultiTaskLandmarkUNetCustom(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805bb3d8-8c07-4d6f-97f0-b484508b28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters:  450266\n"
     ]
    }
   ],
   "source": [
    "# count the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75fb31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1, 1, 256, 256)\n",
    "image /= image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a786b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "out = model(image, task_id=3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d1ba8-7d69-43a5-9ebd-952131706010",
   "metadata": {},
   "source": [
    "# Test Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9565c527-b628-4f18-b3b1-262020761216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    trainer_v_landmarks_single_task,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26fe04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                        | Params\n",
      "----------------------------------------------------------\n",
      "0 | model     | MultiTaskLandmarkUNetCustom | 18.0 M\n",
      "1 | train_mse | MeanSquaredError            | 0     \n",
      "2 | val_mse   | MeanSquaredError            | 0     \n",
      "----------------------------------------------------------\n",
      "450 K     Trainable params\n",
      "17.5 M    Non-trainable params\n",
      "18.0 M    Total params\n",
      "71.992    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "/home/samehr/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "trainer_v_landmarks_single_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b032c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader\n",
    "\n",
    "task_config = PARAMS.TRAIN.SINGLE_TASK\n",
    "task_id = task_config.TASK_ID\n",
    "batch_size = task_config.BATCH_SIZE\n",
    "\n",
    "train_dataloader = create_dataloader(\n",
    "    task_id=task_id,\n",
    "    batch_size=batch_size,\n",
    "    split='train',\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf14e8a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_batch, sample_batched \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i_batch, sample_batched[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(),\n\u001b[1;32m      3\u001b[0m           sample_batched[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_landmarks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/cephal/cvmt/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['v_landmarks'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169e927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
